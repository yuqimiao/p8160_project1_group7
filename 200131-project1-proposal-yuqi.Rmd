---
title: "p8160-project1"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r,include=FALSE}


library(tidyverse)
library(patchwork)

result_less_weak_corr_n200 = readRDS("./R_script/result_less_weak_corr_n200.rds")
result_less_weak_corr_n1000 = readRDS("./R_script/result_less_weak_corr_n1000.rds")
result_less_weak_corr_n5000 = readRDS("./R_script/result_less_weak_corr_n5000.rds")

result_less_weak_corr_n200$percentage

beta_less_weak_corr_n200 = result_less_weak_corr_n200$measures
beta_less_weak_corr_n1000 = result_less_weak_corr_n1000$measures
beta_less_weak_corr_n5000 = result_less_weak_corr_n5000$measures

beta_less_weak_corr_df <-
  bind_rows(beta_less_weak_corr_n200, 
            beta_less_weak_corr_n1000,
            beta_less_weak_corr_n5000) %>% 
  rename(idx_sim = idx) %>% 
  mutate(true_beta = (true_beta),
         coefficient = (coefficient),
         n = as.factor(n)) %>% 
  mutate(bias2 = (coefficient - true_beta)^2,
         bias = coefficient - true_beta)
beta_less_weak_corr_df$idx_beta = rep(1:100, each = 2)
beta_less_weak_corr_df = beta_less_weak_corr_df %>% 
  select(idx_sim, idx_beta, everything()) %>% 
  mutate(selected = if_else(coefficient==0, 0, 1),
         is_nonzero = if_else(type == "null", 0, 1),
         is_TP = if_else(type!="null"&coefficient!=0, 1, 0),
         is_TN = if_else(type=="null"&coefficient==0, 1, 0)
         )

# names(beta_less_weak_corr_df)
# unique(beta_less_weak_corr_df$idx_sim)
# unique(beta_less_weak_corr_df$true_beta) %>% sort()
# unique(beta_less_weak_corr_df$type) 

## plots of beta estimates
beta_less_weak_corr_df %>% 
  filter(n == 200) %>% 
  group_by(idx_sim, method, type, n) %>% 
  summarise(beta_rmse = sqrt(mean(bias2)),
            beta_bias_mean = mean(bias)) %>% 
  ggplot(aes(x = method, y = beta_rmse, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

beta_less_weak_corr_df %>% 
  filter(n != 200) %>% 
  group_by(idx_sim, method, type, n) %>% 
  summarise(beta_rmse = sqrt(mean(bias2)),
            beta_bias_mean = mean(bias)) %>% 
  ggplot(aes(x = method, y = beta_rmse, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

beta_less_weak_corr_df %>% 
  ggplot(aes(x = method, y = bias, color = type)) +
    geom_boxplot()


confusion_matrix_df = 
 beta_less_weak_corr_df %>% 
  group_by(idx_sim, method, n) %>% 
  summarize(n_selected = sum(selected),
            n_features = n(),
            n_nonzero = sum(is_nonzero),
            n_correct = sum(correct),
            n_TP = sum(is_TP),
            n_TN = sum(is_TN)) %>% 
  mutate(sensitivity = n_TP/n_nonzero,
         specificity = n_TN/(n_features-n_nonzero),
         accuracy = n_correct/n_features,
         precision = n_TP/n_selected,
         F1_score = 2/(1/precision + 1/sensitivity))

confusion_matrix_type_df = 
 beta_less_weak_corr_df %>% 
  group_by(idx_sim, method, n, type) %>% 
  summarize(n_selected = sum(selected),
            n_features = n(),
            n_nonzero = sum(is_nonzero),
            n_correct = sum(correct),
            n_TP = sum(is_TP),
            n_TN = sum(is_TN)) %>% 
  mutate(sensitivity = n_TP/n_nonzero,
         specificity = n_TN/(n_features-n_nonzero),
         accuracy = n_correct/n_features,
         precision = n_TP/n_selected,
         F1_score = 2/(1/precision + 1/sensitivity))

confusion_matrix_df %>% 
  ggplot(aes(x = method, y = sensitivity, color = n)) +
    geom_boxplot()

g4 = confusion_matrix_type_df %>% 
  ggplot(aes(x = method, y = sensitivity, color = type)) +
    geom_boxplot() +
    facet_grid(~n) +
  ggtitle("5:1:4 sensitivity")

g5 = confusion_matrix_df %>% 
  ggplot(aes(x = method, y = specificity)) +
    geom_boxplot() +
    facet_grid(~n)+
  ggtitle("5:1:4 specificity")

confusion_matrix_df %>% 
  ggplot(aes(x = method, y = accuracy, color = n)) +
    geom_boxplot()

confusion_matrix_type_df %>% 
  ggplot(aes(x = method, y = accuracy, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

g6 =confusion_matrix_df %>% 
  ggplot(aes(x = method, y = F1_score, color = n)) +
    geom_boxplot()+
  ggtitle("5:1:4 F1 score")

g15 = confusion_matrix_type_df %>% 
  ggplot(aes(x = method, y = sensitivity, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

confusion_matrix_df %>% 
    ggplot(aes(x = method, y = n_selected, color = n)) +
    geom_boxplot()

result_more_weak_corr_n200 = readRDS("./R_script/result_more_weak_corr_n200.rds")
result_more_weak_corr_n1000 = readRDS("./R_script/result_more_weak_corr_n1000.rds")
result_more_weak_corr_n5000 = readRDS("./R_script/result_more_weak_corr_n5000.rds")

result_more_weak_corr_n200$percentage

beta_more_weak_corr_n200 = result_more_weak_corr_n200$measures
beta_more_weak_corr_n1000 = result_more_weak_corr_n1000$measures
beta_more_weak_corr_n5000 = result_more_weak_corr_n5000$measures

beta_more_weak_corr_df <-
  bind_rows(beta_more_weak_corr_n200, 
            beta_more_weak_corr_n1000,
            beta_more_weak_corr_n5000) %>% 
  rename(idx_sim = idx) %>% 
  mutate(true_beta = abs(true_beta),
         coefficient = abs(coefficient),
         n = as.factor(n)) %>% 
  mutate(bias2 = (coefficient - true_beta)^2,
         bias = coefficient - true_beta)
beta_more_weak_corr_df$idx_beta = rep(1:100, each = 2)
beta_more_weak_corr_df = beta_more_weak_corr_df %>% 
  select(idx_sim, idx_beta, everything()) %>% 
  mutate(selected = if_else(coefficient==0, 0, 1),
         is_nonzero = if_else(type == "null", 0, 1),
         is_TP = if_else(type!="null"&coefficient!=0, 1, 0),
         is_TN = if_else(type=="null"&coefficient==0, 1, 0)
         ) 

# names(beta_more_weak_corr_df)
# unique(beta_more_weak_corr_df$idx_sim)
# unique(beta_more_weak_corr_df$true_beta) %>% sort()
# unique(beta_more_weak_corr_df$type) 
# table(beta_more_weak_corr_df$type, beta_more_weak_corr_df$method)

## plots of beta estimates
beta_more_weak_corr_df %>% 
  filter(n == 200) %>% 
  group_by(idx_sim, method, type, n) %>% 
  summarise(beta_rmse = sqrt(mean(bias2)),
            beta_bias_mean = mean(bias)) %>% 
  ggplot(aes(x = method, y = beta_bias_mean, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

beta_more_weak_corr_df %>% 
  filter(n != 200) %>% 
  group_by(idx_sim, method, type, n) %>% 
  summarise(beta_rmse = sqrt(mean(bias2)),
            beta_bias_mean = mean(bias)) %>% 
  ggplot(aes(x = method, y = beta_bias_mean, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

beta_more_weak_corr_df %>% 
  filter(n != 200) %>% 
  group_by(idx_sim, method, type, n) %>% 
  summarise(beta_rmse = sqrt(mean(bias2)),
            beta_bias_mean = mean(bias)) %>% 
  ggplot(aes(x = method, y = beta_rmse)) +
    geom_boxplot() +
    facet_grid(~n)



confusion_matrix_df2 = 
  beta_more_weak_corr_df %>% 
  group_by(idx_sim, method, n) %>% 
  summarize(n_selected = sum(selected),
            n_features = n(),
            n_nonzero = sum(is_nonzero),
            n_correct = sum(correct),
            n_TP = sum(is_TP),
            n_TN = sum(is_TN)) %>% 
  mutate(sensitivity = n_TP/n_nonzero,
         specificity = n_TN/(n_features-n_nonzero),
         accuracy = n_correct/n_features,
         precision = n_TP/n_selected,
         F1_score = 2/(1/precision + 1/sensitivity))
  


confusion_matrix_type_df2 = 
 beta_more_weak_corr_df %>% 
  group_by(idx_sim, method, n, type) %>% 
  summarize(n_selected = sum(selected),
            n_features = n(),
            n_nonzero = sum(is_nonzero),
            n_correct = sum(correct),
            n_TP = sum(is_TP),
            n_TN = sum(is_TN)) %>% 
  mutate(sensitivity = n_TP/n_nonzero,
         specificity = n_TN/(n_features-n_nonzero),
         accuracy = n_correct/n_features,
         precision = n_TP/n_selected,
         F1_score = 2/(1/precision + 1/sensitivity))

g7 =confusion_matrix_df %>% 
  ggplot(aes(x = method, y = sensitivity, color = n)) +
    geom_boxplot()+
  ggtitle("5:3:2 sensitivity")

g16 = confusion_matrix_type_df2 %>% 
  ggplot(aes(x = method, y = sensitivity, color = type)) +
    geom_boxplot() +
    facet_grid(~n)+
  ggtitle("5:3:2 sensitivity by type")


g8 = confusion_matrix_df %>% 
  ggplot(aes(x = method, y = specificity)) +
    geom_boxplot() +
    facet_grid(~n)+
  ggtitle("5:3:2, specificity")

confusion_matrix_df %>% 
  ggplot(aes(x = method, y = accuracy, color = n)) +
    geom_boxplot()

confusion_matrix_type_df2 %>% 
  ggplot(aes(x = method, y = accuracy, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

g9 = confusion_matrix_df %>% 
  ggplot(aes(x = method, y = F1_score, color = n)) +
    geom_boxplot()+
  ggtitle("5:3:2, F1_score")

confusion_matrix_type_df2 %>% 
  ggplot(aes(x = method, y = F1_score, color = type)) +
    geom_boxplot() +
    facet_grid(~n)

confusion_matrix_df %>% 
    ggplot(aes(x = method, y = n_selected, color = n)) +
    geom_boxplot()


result_missing_weak_n200 = readRDS("./R_script/result_missing_weak_n200.rds")
result_missing_weak_n1000 = readRDS("./R_script/result_missing_weak_n1000.rds")
result_missing_weak_n5000 = readRDS("./R_script/result_missing_weak_n5000.rds")

result_missing_weak_n200$percentage

beta_missing_weak_n200 = result_missing_weak_n200$measures
beta_missing_weak_n1000 = result_missing_weak_n1000$measures 
beta_missing_weak_n5000 = result_missing_weak_n5000$measures 

beta_missing_weak_n200_new = result_missing_weak_n200$measures_new
beta_missing_weak_n1000_new = result_missing_weak_n1000$measures_new
beta_missing_weak_n5000_new = result_missing_weak_n5000$measures_new

beta_missing_weak <-
  bind_rows(beta_missing_weak_n200, 
            beta_missing_weak_n1000,
            beta_missing_weak_n5000) %>% 
  filter(type %in% c("strong", "null")) %>% 
  rename(idx_sim = idx) %>% 
  mutate(n = as.factor(n))   

beta_missing_weak_new <-
  bind_rows(beta_missing_weak_n200_new, 
            beta_missing_weak_n1000_new,
            beta_missing_weak_n5000_new) %>% 
  rename(idx_sim = idx) %>% 
  mutate(n = as.factor(n)) %>%  
  rename(coefficient_new = coefficient)

beta_missing_weak_combined  = 
  beta_missing_weak %>% 
  mutate(coefficient_new = beta_missing_weak_new$coefficient_new) %>% 
  gather(key = "coef_type", value = "coef", c(coefficient, coefficient_new)) %>% 
  mutate(bias = coef - true_beta) 

temp_df = 
  beta_missing_weak_combined %>% 
  filter(n != "200") %>% 
  filter(type == "strong") %>% 
  group_by(idx_sim, method, n, coef_type) 

temp_df %>% 
  summarize(beta_rmse = sqrt(mean((coef-true_beta)^2)) ) %>% 
  ggplot(aes(x = method, y = beta_rmse, color = coef_type)) +
    geom_boxplot() +
    facet_grid(~n)

temp_df %>% 
  filter(true_beta == unique(temp_df$true_beta)[3]) %>% 
  ggplot(aes(x = method, y = bias, color = coef_type)) +
    geom_boxplot()



beta_missing_weak_combined %>% 
  filter(n != "200") %>% 
  filter(type == "strong") %>% 
  group_by(idx_sim, method, n, coef_type) %>% 
  summarize(beta_bias = mean(bias) ) %>% 
  ggplot(aes(x = method, y = beta_bias, color = coef_type)) +
    geom_boxplot() +
    facet_grid(~n)

bind_cols(beta_missing_weak, beta_missing_weak_new) %>% 
  mutate(coef_diff = coefficient_new - coefficient) %>% 
  filter(n != "200") %>%
  filter(type == "strong") %>% 
  group_by(idx_sim, method, n) %>% 
  summarize(mean_coef_diff = mean(coef_diff) ) %>% 
  ggplot(aes(x = method, y = mean_coef_diff)) +
    geom_boxplot() +
    facet_grid(~n) 

g13 = confusion_matrix_df %>% 
  ggplot(aes(x = method, y = n_selected,color = n))+
  geom_boxplot()+
  ggtitle("5:1:4 model size")
    
g14 = confusion_matrix_df2 %>% 
  ggplot(aes(x = method, y = n_selected, color = n)) +
    geom_boxplot()+
    ggtitle("5:3:2 model size")
```




# 1 Objectives 
Design a simulation study to investigate and illustrate 
how well each of the two methods in identifying weak and strong predictors;
how missing “weak” predictors impacts the parameter estimations. 
To do so, you need to simulate data with a combination of strong'',weak-but-correlated” and “weak-and- independent” predictors. 

# 2 Statistical methods to be studied 

## 2.1 Step-wise forward method

Starting with the empty model, and iteratively adds the variables that best improves the model fit. That is often done by sequentially adding predictors with the largest reduction in AIC. For linear models,
$$AIC=n\ln(\sum_{i=1}^n{(y_i-\widehat{y_i})}^2/n)+2p$$
where $\widehat{y_i}$ is the fitted values from a model, and $p$ is the dimension of the model (i.e.,number of predictors plus 1).

## 2.2 Automated LASSO regression LASSO

another popular method for variable selection. It estimates the model parameters by optimizing a penalized loss function:

$$\underset\beta{min}\frac{1}{2n}\sum_{i=1}^n(y_i-x_i\beta)^2+\lambda\sum_{k=1}^p|\beta_k|$$
where $\lambda$ is a tunning parameter. Cross-validation (CV) is the most common selection criteria for LASSO.


# 3 Scenarios to be investigated 

* weak-to-strong predictor ratio;
In this study, we want to simulate the situation where the predictors are having underlying correlation, we define certain scenarios to compute the robustness of 2 variable selection methods.

3.2 tuning parameter
Sample size: 
1) n = 200
2) n = 1000
3) n = 5000
Strength predictors ratio:
 The total number of predictors are 200, where 100 are noise predictors. Here we consider 2 strength predictors ratio ( r: strength: weak_but_correlated: weak_and_independent )
	1) r = 5:1:4
	2) r = 5:3:2
6 combinations of above parameters are the scenario to be studied 

 

# 4 Methods for generating data 

 the simulated population needs to meet following characteristics：Firstly,the expectation of outcome variable is the linear combination of predictors with an constant-variance error term; Secondly, the predictors are mutually correlated, Thirdly, the parameters are distinctly correlated with outcome, which is classified by a critical value. The detailed definitions of data generating funcion parameters are as follows:

## 4.1 Distribution of population

True distribution of outcome variable is defined as 
$$Y\sim N(\boldsymbol X\boldsymbol \beta, \sigma^2)$$

Where $\boldsymbol X$ is the predictor matrix, $\boldsymbol \beta$ is the parameters vector and $\sigma^2$ is the constant variance in normal distribution.

## 4.2 Predictor strenghth

The strength of parameters are defined by following criterias:

Definition of strong predictors:

$$S_1 = \left\{j:|\beta_j|>c\sqrt{\log(p)/n},some\ c>0,1\leq j\leq p\right\}$$

Definition of weak but correlated predictors:

  $$S_2 = \left\{j:0<|\beta_j|\leq c\sqrt{\log(p)/n},some\ c>0,corr(X_i,X_{j'})\neq0,some\ j'\in S_1,1\leq j\leq p\right\}$$
Definition of weak and independent predictors:

$$S_3 = \left\{j:0<|\beta_j|\leq c\sqrt{\log(p)/n},some\ c>0,corr(X_i,X_{j'})=0,all\ j'\in S_1,1\leq j\leq p\right\}$$
Definition of noise:\
$$S_4 = \left\{j:|\beta_j| = 0,1\leq j\leq p\right\}$$

Parameters can be generated by 2 ways. Firstly, given beta by artificial setting, and secondly, by defining constant c, parameters are generated uniformly with in above ranges.

## 4.3 Parameter correlation

The data of predictors are generated by R function `mvrnorm`. In order to change the correlation between different predictors, a pre-defined covariance matrix is passed to `mvrnorm` function. Here we consider a certain scenario: the strong variables and a certain ratio of weak variables are indepent to other variables, and other weak predictors are correlated with 1 specific strong variable. To ensure the positive definite attribute of covariance matrix, we restrict one strong predictor can only be correlated with one weak predictor.




# 5 performance measures 
## 5.1 Predictor identification performance

1. define specification, sensitivity and interpretation
2. put plots and compare in terms of 
  * lasso/weak
  * 6 scenario
  
In order to compare the identification performance for these 2 methods, we regard the identification as a classification problem, where the signal predictors are defined as positive and null predictors are defined as negative. 3 indicators have been established:

$$ recall = sensitivity=\left(\frac{True\ positive}{True\ positive+False\ negative}\right)$$
$$specificity=\left(\frac{True\ negative}{True\ negative+False\ positive}\right)$$


$$F_1=\left(\frac{2}{recall^{-1}+precision^{-1}}\right)$$
Where precision is defined as above, and recall is defined as

$$precision=\left(\frac{True\ positive}{True\ positive+False\ positive}\right)$$


## 5.2 estimation performance

To compare the paramater estimation performance, 3 indicators were calculated to evaluate the estimation: bias is calculated as the mean difference between true parameter and estimated parameters, variance is defined as the variance of the estimated parameter among simulation, and MSE

By tuning parameters differently, robustness of the variable selection methods were also evaluated through the above indicators.


# 6 simulation results. 

## 6.1  identification performance

```{r,echo = FALSE, fig.cap="F1 score for 2 variable selection methods in 6 settings", fig.height=7,fig.width= 7}
g6/g9
```

F1 score is used as an indicater for an overall performance assessment for identification signals. There is no significant difference between this two methods when sample size is large. While in high dimensional occasion, stepwise has a poor performance.

```{r,echo = FALSE,fig.cap="Model size for 2 variable selection methods in 6 settings", fig.height=7,fig.width= 7}
g13/g14
```

In terms of variable selection size, overal, lasso tend to choose more predictors while stepwise tend to choose less overall, especially in the settings in this report, where there are 100 true signals and 100 noises, lasso tend to select more predictors than true occasion, while stepwise tends to choose less. When comparing in terms of sample size, lasso performs stable in both high dimensional and normal scenario(where n = p), but stepwise tend to break when the ratio between number of parameters and sample size is large. 

```{r, echo = FALSE, fig.cap="Sensitivity for 2 variable selection methods in 6 settings", fig.height=7,fig.width= 7}
 (g4+g7)/(g15+g16)
```

When comparing the specificity of these 2 methods, lasso performs better overall since more predictors are chosen, when observing sensitivity in terms of predictors strength, both methods perform well for strong predictors when sample size is large enough, while for weak predictors, lasso still has a better performance



```{r,echo = FALSE, fig.cap="Specificity for 2 variable selection methods in 6 settings", fig.height=7,fig.width= 7}
g5/g8
```







6.2 Parameter Estimation 
* bias

$$\frac{1}{p}\sum_{i=1}^p(\hat{\boldsymbol\beta}-\boldsymbol\beta)$$


* variance 

$$\frac{1}{p}\sum_{i=1}^p(\hat{\boldsymbol\beta}-\overline{\boldsymbol \beta})^2$$

* MSE

$$\frac{1}{p}\sum_{i=1}^p(\hat{\boldsymbol\beta}-\boldsymbol\beta)^2$$

1. bias


2. rmse


3. comparision of missing vs no missing


